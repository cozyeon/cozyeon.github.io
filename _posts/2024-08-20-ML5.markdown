---
layout: post
title: "[혼자 공부하는 머신러닝 + 딥러닝] Chapter5. 트리 알고리즘"
date: 2023-08-20
categories: "머신러닝"
---

# **5-1. 결정 트리**
### 로지스틱 회귀로 와인 분류하기

#### # 데이터 파악

```python
import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')

wine.head()

wine.info()

wine.describe()
```

-> head(): 처음 5개 샘플 확인 (0~4행 출력)   
-> info(): 데이터프레임의 각 열의 데이터 타입과 누락된 데이터 있는지 확인하는 데 유용   
-> describe(): 각 열에 대한 평균, 표준편차, 최소, 1사분위수, 중간값, 2사분위수, 3사분위수, 최대 출력   

#### # 훈련세트와 테스트세트로 나누기

```python
data = wine[['alcohol', 'sugar', 'pH']].to_numpy() # 판다스 데이터프레임을 넘파이 배열로 바꿈
target = wine['class'].to_numpy()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

print(train_input.shape, test_input.shape)
```
-> test_size: 테스트 세트의 비율 설정, 기본값 25%

#### # 로지스틱 회귀 모델 훈련

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target)) #출력: 0.78...
print(lr.score(test_scaled, test_target)) #출력: 0.77...

print(lr.coef_, lr.intercept_) 
```

-> 훈련 세트와 테스트 세트의 점수가 모두 낮으므로 과소적합   
-> 계수와 절편으로 모델을 규정할 수 있지만 학습의 결과를 설명하기 어려움.


### 결정 트리
- 리프 노드에서 가장 많은 클래스가 예측 클래스가 됨.

#### # 모델 훈련, 성능 평가

```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target)) #출력: 0.99...
print(dt.score(test_scaled, test_target)) #출력: 0.85...
```
-> 훈련 세트 점수는 매우 높지만 테스트 세트 점수는 비교적 낮으므로 과대적합   

#### # 결정 트리 그림 출력

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
-> plot_tree() 함수: 결정 트리를 트리 그림으로 출력

#### # 매개변수 설정으로 원하는 트리 출력

```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

-> max_depth: 트리의 깊이 제한    
-> filled 매개변수: 클래스에 맞게 노드의 색 칠함.   
-> feature_names 매개변수: 특성의 이름 전달   
-> filled=True: 클래스마다 색깔 부여, 클래스 비율에 비례하여 색이 진해짐.   
![node](/assets/img/node.jpg)   
![node_explain](/assets/img/node_explain.jpg)

### 불순도

- 종류: 지니 불순도, 엔트로피 불순도
    - 지니 불순도
        - DecisionTreeClassifier 클래스의 criterion 매개변수(노드에서 데이터를 분할하는 기준)의 기본값
        - >$ 지니\,불순도 = 1 - (음성\,클래스\,비율^2 + 양성\,클래스\,비율 ^2) $
        - 어떤 노드의 두 클래스 비율이 1/2씩 -> 지니 불순도 0.5 => 최악
        - 노드에 하나의 클래스만 존재 -> 지니 불순도 0 => 순수 노드


    - 엔트로피 불순도
        - DecisionTreeClassifier 클래스에서 criterion='entropy'를 지정하여 사용
        - >$엔트로피\,불순도 =\\ -음성\,클래스\,비율 \times log_2(음성\,클래스\,비율)\\-양성\,클래스\,비율 \times log_2(양성\,클래스\,비율)$

- 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장시킴.
- 정보이득: 부모 노드와 자식 노드의 불순도 차이   
>$부모 \,노드와 \,자식 \,노드의 \,불순도 \,차이=\\    
부모의\,불순도\\ - (왼쪽\,노드\,샘플\,수 / 부모의\,샘플\,수) \times 왼쪽\,노드\,불순도\\ - (오른쪽\,노드\,샘플\,수 / 부모의\,샘플수) \times 오른쪽\,노드\,불순도 $

### 가지치기
: 트리의 최대 깊이 지정

#### # 트리의 최대 깊이 지정

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target)) #출력: 0.845...
print(dt.score(test_scaled, test_target)) #출력: 0.841...
```
-> max_depth 매개변수: 트리의 최대 깊이 지정

#### # 트리 그래프 출력

```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![tree_graph](/assets/img/tree_graph.jpg)   
-> 당도가 음수???   
=> 특성값의 스케일은 결정 트리 알고리즘에 아무런 영향을 미치지 않으므로 표준화 전처리 할 필요 없음!

#### # 전처리 전의 샘플로 결정 트리 모델 다시 훈련

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target)) #출력: 0.845...
print(dt.score(test_input, test_target)) #출력: 0.841...

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![tree_graph_pre](assets/img/tree_graph_pre.jpg)

#### # 특성 중요도 계산
특성 중요도: 어떤 특성이 가장 유용한지 나타냄.

```python
print(dt.feature_importances_)
```
-> 특성 중요도는 결정 트리 모델의 feture_importances_ 속성에 저장, 출력된 값들을 모두 더하면 1이 됨.

# **5-2. 교차 검증과 그리드 서치**

### 검증 세트
: 테스트 세트 대신 모델의 과대/과소 적합 여부를 판단하는 데 사용하기 위해 훈련 세트에서 떼어 낸 세트
- 매개변수를 바꿔가며 검증 세트로 테스트하여 가장 좋은 모델을 고름.   
-> 이 매개변수를 사용해 훈련 세트와 검증 세트를 합쳐 전체 훈련 데이터에서 모델 다시 훈련   
-> 마지막에 테스트 세트에서 최종 점수 평가

#### # 데이터 준비하기

```python
import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy() # class가 아닌 열은 특성 배열에 저장
target = wine['class'].to_numpy() # class 열을 타깃으로 사용
```
#### # 훈련, 검증, 테스트 세트로 나누기

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42) # 훈련 세트 + 테스트 세트

sub_input, val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42) # sub 훈련 세트 + 검증 세트

print(sub_input.shape, val_input.shape)
```

#### # 모델 훈련, 평가

```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)

print(dt.score(sub_input, sub_target)) #출력: 0.99...
print(dt.score(val_input, val_target)) #출력: 0.86...
```
-> 훈련 세트에 과대적합


### 교차 검증
: 검증 세트를 떼어 내어 평가하는 과정 여러 번 반복
- 많은 훈련 데이터를 사용할수록 좋은 모델이 만들어짐.   
-> 검증 세트를 만들면서 훈련 세트가 줄어듦.   
 => 교차 검증 이용하면 됨.   
 - k-폴드 교차 검증: 훈련 세트를 k개의 부분으로 나누어 교차 검증을 수행.   
 ![cross_validation](/assets/img/cross_validation.jpg)

#### # 교차 검증 수행

```python
from sklearn.model_selection import cross_validate

scores = cross_validate(dt, train_input, train_target)
print(scores)
```

>cross_validate() 함수
>-
>- 평가할 모델 객체, 훈련 세트 전체를 전달받아 교차 검증 수행.
>- 기본적으로 5-폴드 교차 검증 수행     
>- fit_time, score_time, test_score 키를 가진 딕셔너리를 반환
>   - fit_time: 모델을 훈련하는 시간
>   - score_time: 모델을 검증하는 시간
>   - test_score: 검증 폴드의 점수, 이 5개의 점수를 평균하여 교차 검증의 최종 점수를 구함.
>- 훈련 세트를 섞어 폴드를 나누지 않음.   
>-> 훈련 세트 섞으려면 분할기 지정해야 함.
>   - 회귀 모델일 경우 KFold 분할기, 분류 모델일 경우 StratifiedKFold 사용

#### # 교차 검증 점수 구하기

```python
import numpy as np

print(np.mean(scores['test_score']))
```

#### # 분할기 지정
분할기: 교차 검증에서 폴드를 어떻게 나눌지 결정. 

```python
from sklearn.model_selection import StratifiedKFold

scores = cross_validate(dt, train_input, train_target, cv=StratifiedKFold())
print(np.mean(scores['test_score']))
```
-> 우리는 train_test_split() 함수로 전체 데이터를 섞은 후 훈련 세트를 준비했으므로 앞서 수행한 교차 검증은 위의 코드와 동일함.

#### # 훈련 세트를 섞은 후 10-폴드 교차 검증을 수행하려면

```python
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))
```
-> n_splits 매개변수: 몇(k) 폴드 교차 검증을 할 지 정함.

### 하이퍼파라미터 튜닝
- 모델 파라미터: 머신러닝 모델이 학습하는 파라미터
- 하이퍼파라미터: 모델이 학습할 수 없어서 사용자가 지정해야만 하는 파라미터   
-> 클래스나 메서드의 매개변수로 표현됨.
- 하이퍼파리미터 튜닝 과정:      
    1. 라이브러리가 제공하는 기본값으로 모델 훈련      
    2. 검증 세트의 점수나 교차 검증을 통해 매개변수를 조금씩 수정
    - 매개변수는 서로 영향을 미치기 때문에 매개변수들을 동시에 수정해가며 최적의 값을 찾아야 함.


### 그리드 서치
: 매개변수들을 동시에 수정하며 최적의 값을 찾는 방법

#### # 탐색할 매개변수와 값 지정

```python
from sklearn.model_selection import GridSearchCV

params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
```
-> 사이킷런의 GridSearchCV: 하이퍼파라미터 탐색과 교차 검증을 한 번에 수행, cross_validate() 호출할 필요X

#### # 그리드 서치 객체 생성, 그리드 서치 수행

```python
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)

gs.fit(train_input, train_target)
```
-> min_impurity_decrease 값을 바꿔가며 총 5번 (탐색할 값이 5개) 실행   
-> cv 매개변수 기본값은 5 -> 탐색하는 값마다 5-폴드 교차 검증 수행 -> 5X5=25개의 모델을 훈련   
-> n_jobs 매개변수: 병렬 실행에 사용할 CPU 코어 수 지정, 기본값은 1, -1로 지정하면 모든 코어 사용


#### # 자동으로 최적의 매개변수로 다시 모델 훈련

```python
dt = gs.best_estimator_
print(dt.score(train_input, train_target))

print(gs.best_params_)
```
-> 검증 점수가 가장 높은 모델의 매개변수 조합으로 전체 훈련 세트에서 자동으로 다시 모델 훈련   
-> gs 객체의 best_estimator_ 속성: 최적의 매개변수 조합으로 훈련된 모델이 저장되어 있음.   
-> gs 객체의 best_paramas_ 속성: 최적의 매개변수가 저장되어 있음.


#### #교차 검증의 평균 점수를 통해 최적의 매개변수 출력

```python
print(gs.cv_results_['mean_test_score'])

best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
```
-> np.argmax() 함수: 가장 큰 값의 인덱스 추출


#### # 더 복잡한 매개변수 조합 탐색

```python
params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
          'max_depth': range(5, 20, 1),
          'min_samples_split': range(2, 100, 10)
          }
```
-> 넘파이 arrange() 함수: 첫 번째 매개변수 ~ 두 번째 매개변수 동안 세 번째 매개변수를 계속 더한 배열을 만듦. (두 번째 매개변수 포함 X)    
-> 파이썬 range() 함수: 넘파이 arrange() 와 비슷하지만 정수만 사용 가능

```python
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)

print(gs.best_params_)
```

-> 최상의 매개변수 조합 확인

```python
print(np.max(gs.cv_results_['mean_test_score']))
```
-> 최상의 교차 검증 점수 확인

### 랜덤 서치
: 매개변수 값의 목록을 전달하는 것이 아니라 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달.
- 랜덤 서치를 사용하면 좋은 경우
    1. 매개변수 값이 수치일 때 값의 범위나 간격을 미리 정하기 어려울 수 있음.
    2. 너무 많은 매개 변수 조건이 있어 그리드 서치 수행 시간이 오래 걸릴 수 있음.   

> scipy
>- 
>- 파이썬의 핵심 과학 라이브러리 중 하나.
>- 적분, 보간, 선형 대수, 확률 등을 포함한 수치 계산 전용 라이브러리

#### # uniform, randit
uniform 클래스: 주어진 범위에서 고르게 실수값을 뽑음.   
randit 클래스: 주어진 범위에서 고르게 정수값을 뽑음.

```python
from scipy.stats import uniform, randint
```

#### # 탐색할 매개변수의 딕셔너리 만들기

```python
params = {'min_impurity_decrease': uniform(0.0001, 0.001),
          'max_depth': randint(20, 50),
          'min_samples_split': randint(2, 25),
          'min_samples_leaf': randint(1, 25),
          }
```

#### # 랜덤 서치 객체 생성, 랜덤 서치 수행

```python
from sklearn.model_selection import RandomizedSearchCV

gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params,
                        n_iter=100, n_jobs=-1, random_state=42)
gs.fit(train_input, train_target)

print(gs.best_params_)

print(np.max(gs.cv_results_['mean_test_score']))

dt = gs.best_estimator_

print(dt.score(test_input, test_target))
```
-> n_iter 매개변수: 샘플링 횟수 지정

# **5-3. 트리의 앙상블**

### 정형 데이터 VS 비정형 데이터
정형 데이터
- 어떤 구조로 되어 있는 데이터   
- CSV, 데이터베이스, 엑셀에 저장하기 쉬움.   

비정형 데이터
- 데이터베이스나 엑셀로 표현하기 어려운 데이터
- 텍스트 데잍, 사진, 음악 등

### 앙상블 학습
- 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘
- 대부분 결정 트리를 기반으로 한 알고리즘
- 종류: 랜덤 포레스트, 엑스트라 트리, 그레이디언트 부스팅, 히스토그램 기반 그레이디언트 부스팅

### 랜덤 포레스트
: 결정 트리를 랜덤하게 만들고, 각 결정 트리의 예측을 사용해 최종 예측을 만듦. 
1. 훈련 데이터를 통해 부트스트랩 샘플을 훈련 세트의 크기와 같게 만듦.
2. 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾음.   
-> 전체 특성 개수의 제곱근만큼의 특성을 선택    
-> 사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이 방식으로 훈련    
![randomforest](/assets/img/randomforest.jpg)     
3. 분류 - 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼음.   
회귀 - 단순히 각 트리의 예측을 평균.

- 랜덤하게 선택한 샘플과 특성을 사용하므로 훈련 세트에 과대적합되는 것을 막아줌.

> 부트스트랩 샘플: 샘플을 하나씩 뽑을 때 뽑았던 샘플을 다시 넣고 뽑는 방식을 반복해서 중복된 샘플을 허용하여 만든 샘플

#### # 와인 데이터 준비

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

wine = pd.read_csv('https://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```

#### # 교차 검증 수행

```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

#### # 특성 중요도 계산

```python
rf.fit(train_input, train_target) # 랜덤 포레스트 모델을 훈련 세트에 훈련
print(rf.feature_importances_)
```
-> 랜덤 포레스트는 DecesionTreeClassifier가 제공하는 중요한 매개변수 모두 제공

#### # 자체적으로 모델을 평가하는 점수 계산
OOB 샘플 (부트스트랩 샘플에 포함되지 않고 남는 샘플)로 부트스트랩 샘플로 훈련한 모델 평가

```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)

rf.fit(train_input, train_target)
print(rf.oob_score_)
```
-> oob_score = True: 각 결정 트리의 OOB 점수를 평균하여 출력, 기본값 False


### 엑스트라 트리
: 부트스트랩 샘플을 사용하지 않는 랜덤 포레스트
- 각 결정 트리를 만들 때 전체 훈련 세트를 사용
- 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할-> 계산 속도 빠름.   
(= DecisionTreeClassifier의 splitter='random')

#### # 모델의 교차 검증 점수 확인

```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

#### # 특성 중요도 확인

```python
et.fit(train_input, train_target)
print(et.feature_importances_)
```

### 그레이디언트 부스팅
: 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법
- 사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리 100개 사용 -> 과대적합에 강하고 높은 일반화 성능 기대
- 경사 하강법을 사용하여 트리를 앙상블에 추가, 학습률 매개변수로 속도 조절
    - 분류 - 로지스틱 손실 함수 사용
    - 회귀 - 평균 제곱 오차 함수 사용
- 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있음. 
- 순서대로 트리를 추가하므로 훈련 속도가 늘미.


#### # 교차 검증 수행

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#출력: 0.88...  0.87...
```
#### # 학습률과 트리의 개수 증가

```python
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#출력: 0.94... 0.87...
```
-> learning_ rate 기본값은 0.1   
-> 결정 트리 개수를 5배나 늘렸지만 과대적합 잘 억제

#### # 특성 중요도 확인

```python
gb.fit(train_input, train_target)
print(gb.feature_importances_)
```

### 히스토그램 기반 그레이디언트 부스팅
: 그레이디언트 부스팅의 속도와 성능을 개선한 방법
- 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높음.
- 입력 특성을 256개의 구간으로 나눔.   
-> 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있음.
- 256개의 구간 중 하나를 떼어 넣고 누락된 값을 위해 사용.    
-> 입력에 누락된 특성 있더라도 전처리 필요 X
- 기본 매개변수에서 안정적인 성능을 얻을 수 있음.
- 트리의 개수를 지정하는 n_estimators 대신 부스팅 반복 횟수를 지정하는 max_iter 사용

#### # 교차 검증 수행

``` python
# 사이킷런 1.0 버전 아래에서는 다음 라인의 주석을 해제하고 실행하세요.
# from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

#### # 특성 중요도 확인

```python
from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target)

result = permutation_importance(hgb, test_input, test_target, n_repeats=10,
                                random_state=42, n_jobs=-1)
print(result.importances_mean)
# 출력: [0.23... 0.50... 0.26...]
```
-> 다양한 특성을 골고루 잘 평가한다고 예상 가능

#### # 테스트 세트에서의 성능 최종 확인

```python
hgb.score(test_input, test_target) #출력: 0.87...
```
> 사이킷런을 제외하고 히스토그램 기반 그레이디언트 부스팅 알고리즘을 구현한 라이브러리
>-
> ### XGBoost
>- 사이킷런의 cross_validate() 함수와 함께 사용 가능
>- tree_method 매개변수를 'hist'로 지정하여 사용
>
>```python
>from xgboost import XGBClassifier
>
>xgb = XGBClassifier(tree_method='hist', random_state=42)
>scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)
>
>print(np.mean(scores['train_score']), np.mean(scores['test_score']))
>```
>### LightGBM 
>
>```python
>from lightgbm import LGBMClassifier
>
>lgb = LGBMClassifier(random_state=42)
>scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
>
>print(np.mean(scores['train_score']), np.mean(scores['test_score']))
>```