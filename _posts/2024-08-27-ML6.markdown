---
layout: post
title: "[혼자 공부하는 머신러닝 + 딥러닝] Chapter6. 비지도 학습"
date: 2023-08-27
categories: "머신러닝"
---
# **6-1. 군집 알고리즘**
### 비지도 학습
: 타깃이 없을 때 사용하는 머신러닝 알고리즘

### 군집
: 비슷한 샘플끼리 그룹으로 모으는 작업

### 클러스터
: 군집 알고리즘에서 만든 그룹

### <과일 사진으로 과일 분류하기>
-> 타깃을 알고 시작

### 과일 사진 데이터 준비

#### # 과일 사진 데이터 다운받기

```python
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
```
> !: 코랩의 코드 셀에서 "!" 문자로 시작하면 코랩은 이후 명령ㅇ르 파이썬 코드가 아닌 리눅스 셀 명령으로 이해

```python
import numpy as np
import matplotlib.pyplot as plt

fruits = np.load('fruits_300.npy')

print(fruits.shape) #출력: (300, 100, 100)
```
-> 출력: 샘플의 개수, 이미지 높이, 이미지 너비    
![fruit_data_shape](/assets/img/fruit_data_shape.jpg)

#### # 이미지 출력

- 맷플롯립의 imshow(): 넘파이 배열로 저장된 이미지를 그릴 수 있음.
- 맷플롯립의 subplots(): 여러 개의 그래프를 배열처럼 쌓을 수 있도록 함.

```python
print(fruits[0, 0, :]) # 첫 번째 행에 있는 픽셀 100개에 들어 있는 값 출력
```
-> 처음 2개의 인덱스를 0으로 지정하고, 마지막 인덱스는 지정하지 않거나 슬라이싱 연산자    
 => 첫 번쨰 행 모두 선택
```python
plt.imshow(fruits[0], cmap='gray')
plt.show()
```
-> cmap='gray' : 흑백 이미지

```python
plt.imshow(fruits[0], cmap='gray_r')
plt.show()
```
-> cmap = 'gray_r' : 반전된 흑백 이미지

```python
fig, axs = plt.subplots(1, 2) # 1개의 행, 2개의 열로 출력
axs[0].imshow(fruits[100], cmap='gray_r')
axs[1].imshow(fruits[200], cmap='gray_r')
plt.show()
```
-> subplot()의 매개변수: 그래프를 쌓을 행과 열 지정   
-> 반환된 axs는 서브 그래프를 담고 있는 배열


### 픽셀값 분석
- 100 X 100 이미지를 길이가 10000인 1차원 배열로 바꿈    
-> 배열 계산에 편리 (배열을 나눌 때 편리)

#### # 순서대로 100개씩 배열을 나눔
```python
apple = fruits[0:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)
```
-> 두 번째 차원과 세 번째 차원을 10000으로 합침.   
-> 첫 번째 차원 -1로 지정: 자동으로 남은 차원을 할당

```python
print(apple.shape) # 출력: (100, 10000)
```

#### # 과일별로 샘플들의 픽셀 평균값 계산, 분포 확인
- 맷플롯립의 hist(): 히스토그램 출력    
-> alpha 매개변수: 투명도 조절
- 맷플롯립의 legend(): 히스토그램 범례 생성

- mean() 의 axis 매개변수   
-> axis=0: 행 방향으로 계산   
-> axis=1: 열 방향으로 계산

```python
print(apple.mean(axis=1))
```
```python
plt.hist(np.mean(apple, axis=1), alpha=0.8)
plt.hist(np.mean(pineapple, axis=1), alpha=0.8)
plt.hist(np.mean(banana, axis=1), alpha=0.8)
plt.legend(['apple', 'pineapple', 'banana'])
plt.show()
```
![fruit_histogram](/assets/img/fruit_histogram.jpg)   
-> 바나나는 다른 두 과일보다 평균값이 작음.   
-> 사과와 파인애플은 평균값 비슷   

> 객체가 차지하는 영역과 평균값은 비례

#### # 과일별로 각 픽셀의 평균 계산, 분포 확인
- 맷플롯립의 bar(): 막대 그래프 출력

```python
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()
```
![fruit_bar](/assets/img/fruit_bar.jpg)   
-> 사과는 사진 아래쪽으로 갈수록 값이 높아짐.   
-> 파인애플은 비교적 고르게 높음.   
-> 바나나는 중아의 픽셀값이 높음.   

#### # 과일별로 각 픽셀 평균값을 이미지처럼 출력
=> 과일 별로 모든 사진을 하벼 놓은 대표 이미지

```python
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
```
![fruit_represent](/assets/img/fruit_represent.jpg)

### 평균값과 가까운 사진 고르기

#### # 사과 이미지 절댓값 오차 구하기
- 넘파이 abs(): 절댓값 계산
```python
abs_diff = np.abs(fruits - apple_mean)
abs_mean = np.mean(abs_diff, axis=(1,2)) 
print(abs_mean.shape) # 출력: (300,)
```

#### # 절댓값 오차가 가장 작은 샘플 100개 출력
- np.argsort(): 작은 것에서 큰 순서대로 나열하여 개수만큼 반환

```python
apple_index = np.argsort(abs_mean)[:100]
fig, axs = plt.subplots(10, 10, figsize=(10,10))
for i in range(10):
    for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
        axs[i, j].axis('off')
plt.show()
```
![100_apples](/assets/img/100_apples.jpg)

-> figsize 매개변수: 전체 그래프의 크기 지정 / 기본값 (8, 6)    
-> axis('off'): 좌표축 그리지 않음.

# **6-2. k-평균**

### k-평균 알고리즘
: 랜덤하게 클러스터 중심(=센트로이드)을 선택하고 점차 가장 가까운 샘플의 중심으로 이동하는 알고리즘 
1. 무작위로 k개의 클러스터 중심 선정   
2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정   
3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심 변경   
4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복   
![k_mean](/assets/img/k_mean.jpg)

### KMeans 클래스
#### # 데이터 다운로드
```python
!wget https://bit.ly/fruits_300_data -O fruits_300.npy
```

#### # 3차원 배열을 2차원 배열로 변경

```python
import numpy as np

fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)
```
#### # 모델 학습

```python
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)

print(km.labels_)
```
-> n_clusters: 클러스터 개수 지정   
-> 비지도 학습이므로 fit() 매서드에 타깃 데이터 X   
-> 군집된 결과는 KMeans 클래스 객체의 labels_ 속성에 저장   
=> labels의 길이는 샘플 개수   
=> labels의 값은 각 샘플이 어떤 레이블에 해당되는지 나타냄.      
    (n_clusters=3 일 때 0,1,2 중 하나의 값을 가짐.)

#### # 군집 결과 확인

```python
print(np.unique(km.labels_, return_counts=True))
# 출력: (array[0,1,2], dtype=int32), array([91,98,111])
```
-> 첫 번째 클러스터(label 0)이 91개, 두 번쨰 클러스터가 98개, 세번 째 클러스터가 111개의 샘플을 모음. 

#### # 군집 결과 시각화
- draw_fruits():    
(샘플 개수, 너비, 높이)의 3차원 배열을 입력받아 가로로 10개씩 이미지 출력.   
샘플 개수에 따라 행과 열의 개수 계산하여 figsize 지정.   
-> figsize는 ratio 매개변수에 비례하여 커짐. (ratio 기본값 1)

```python
import matplotlib.pyplot as plt

def draw_fruits(arr, ratio=1):
    n = len(arr)    # n은 샘플 개수입니다
    # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다.
    rows = int(np.ceil(n/10))
    # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다.
    cols = n if rows < 2 else 10
    fig, axs = plt.subplots(rows, cols,
                            figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:    # n 개까지만 그립니다.
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()
```
```python
draw_fruits(fruits[km.labels_==0])
```
![apple](/assets/img/apple.jpg)
```python
draw_fruits(fruits[km.labels_==1])
```
```python
draw_fruits(fruits[km.labels_==2])
```

-> 샘플들을 완벽하게 구별해내지는 못함.

### 클러스터 중심

#### # 클러스터 중심 이미지로 출력
-KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluster_centers_ 속성에 저장   
-> 이미지로 출력하려면 100 X 100 크기의 2차원 배열로 변환해야 함.

```python
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```

#### # 인덱스가 100인 샘플의 레이블 탐색

```python
print(km.transform(fruits_2d[100:101]))
# 출력: [[5267.~   8837.~   3393.~ ]]
```
- transform(): 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환    
-> 이 샘플은 거리가 가장 가까운 레이블 2에 속함.

```python
print(km.predict(fruits_2d[100:101]))
# 출력: [2]
```
- predict(): 가장 가까운 클러스터 중심을 예측 클래스로 출력

#### # 인덱스가 100인 샘플 이미지 확인
```python
draw_fruits(fruits[100:101] )
```

#### # 알고리즘이 반복한 횟수
```python
print(km.n_iter_) # 출력: 3
```
-> n_iter_ 속성: 알고리즘이 반복한 횟수 저장


### 최적의 k 찾기
엘보우: 클러스터 개수를 늘려가며 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법

- 이너셔: 클러스터 중심과 클러스터에 속한 샘플 사이의 거리의 제곱 합   
-> 클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지 나타냄.  
-> 클러스터 개수와 크기는 반비례, 클러스터 개수와 이너셔도 반비례

- 클러스터 개수를 증가시키면서 이넛를 그래프로 그리면 감소하는 속도가 꺾이는 지점이 있음. 이 지점부터는 클러스터를 증가시켜도 이너셔의 감소 정도가 크지 않음.

#### # 이너셔 계산
```python
inertia = []
for k in range(2, 7): # 클러스터 개수를 2~6까지 바꿔가며 훈련
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_) # 이너셔 값을 리스트에 추가

plt.plot(range(2, 7), inertia)
plt.xlabel('k')
plt.ylabel('inertia') # 이너셔 값 출력
plt.show()
```
-> inertia 속성: KMeans 클래스는 자동으로 이너셔를 계산해서 이 속성에 저장

![inertial](/assets/img/inertia.jpg)

# **6-3. 주성분 분석**

### 차원과 차원 축소
- 차원: 머신러닝에서 데이터가 가진 속성 (이미지에서는 픽셀의 수)
- 차원 축소: 데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터 크기를 줄이고 지도 학습 모델의 성능을 향상시킬 수 있는 방법   
-> 대표적인 차원 축소 알고리즘: 주성분 분석

### 주성분 분석(PCA)
: 데이터에 있는 분산이 큰 방향을 찾는 것
- 분산: 데이터가 널리 퍼져있는 정도
- 주성분: 분산이 큰 방향을 데이터로 잘 표현하는 벡터   
-> 원소의 개수: 원본 데이터셋에 있는 특성 개수
- 원본 데이터는 주성분을 사용해 차원을 줄일 수 있음.
-> 주성분은 원본 차원과 같고, 주성분으로 바꾼 데이터는 차원이 줄어듦.   
![PCA](/assets/img/pca.jpg)
- 두번째 주성분: 첫 번째 주성분에 수직이고 분산이 가장 큰 방향의 벡터
![second_PCA](/assets/img/2nd_pca.jpg)
- 주성분은 원본 특성의 개수만큼 찾을 수 있음.

### PCA 클래스
#### # 과일 사진 데이터 다운로드하여 넘파이 배열로 적재
```python
!wget https://bit.ly/fruits_300_data -O fruits_300.npy

import numpy as np

fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)
```
#### # 모델 훈련
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=50)
pca.fit(fruits_2d)
```
-> PCA 클래스의 객체를 만들 때 n_components 매개변수에 주성분 개수 지정해야 함.

```python
print(pca.components_.shape) # 출력: (50, 10000)
``` 
-> 첫 번째 차원 = 주성분의 개수, 두 번쨰 차원 = 원본 데이터의 특성 개수

#### # 주성분 이미지로 출력
```python
import matplotlib.pyplot as plt

def draw_fruits(arr, ratio=1):
    n = len(arr)    # n은 샘플 개수입니다
    # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다.
    rows = int(np.ceil(n/10))
    # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다.
    cols = n if rows < 2 else 10
    fig, axs = plt.subplots(rows, cols,
                            figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:    # n 개까지만 그립니다.
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()

draw_fruits(pca.components_.reshape(-1, 100, 100))
```
-> 원본 데이터에서 가장 분산이 큰 방향을 순서대로 나타냄.   
-> 데이터 셋에 있는 어떤 특징을 잡아낸 것   

#### # 특성 개수 줄이기
```python
print(fruits_2d.shape) # 출력: (300, 10000)

fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape) # 출력: (300, 50)
```

### 원본 데이터 재구성
10000개의 특성을 50개로 줄였으므로 어느 정도 손실이 발생. but 상당 부분 재구성 가능

#### # 10000개의 특성으로 복원
```python
fruits_inverse = pca.inverse_transform(fruits_pca)
print(fruits_inverse.shape) # 출력: (300, 10000)
```

#### # 2차원 데이터로 바꿔 100개씩 출력
```python
fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)

for start in [0, 100, 200]:
    draw_fruits(fruits_reconstruct[start:start+100])
    print("\n")
```

### 설명된 분산
: 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값
- 분산 비율을 모두 더하면 50개의 주성분으로 표현하고 있는 총 분산 비율을 얻을 수 있음. 
- PCA 클래스의 explaind_variance_ratio 에 각 주성분의 설명된 분산 비율 기록

#### # 분산 비율의 합
```python
print(np.sum(pca.explained_variance_ratio_)) # 출력: 0.9215...
```
-> 92%가 넘는 분산 유지

#### # 설명된 분산의 비율을 그래프로 시각화
-> 적절한 주성분의 개수를 찾는 데 도움.

```python
plt.plot(pca.explained_variance_ratio_)
```
-> 처음 10개의 주성분이 대부분의 분산을 표현


### 다른 알고리즘과 함께 사용하기
```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

target = np.array([0] * 100 + [1] * 100 + [2] * 100)

from sklearn.model_selection import cross_validate

scores = cross_validate(lr, fruits_2d, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

pca = PCA(n_components=0.5)
pca.fit(fruits_2d)

print(pca.n_components_)

fruits_pca = pca.transform(fruits_2d)
print(fruits_pca.shape)

scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_pca)

print(np.unique(km.labels_, return_counts=True))

for label in range(0, 3):
    draw_fruits(fruits[km.labels_ == label])
    print("\n")

for label in range(0, 3):
    data = fruits_pca[km.labels_ == label]
    plt.scatter(data[:,0], data[:,1])
plt.legend(['apple', 'banana', 'pineapple'])
plt.show()
```